{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is supposed to demonstrate the application of PCA.\n",
    "As a dataset we will use the MNIST database of handwritten digits. The dataset can be found under the follwing link:\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "But instead of manually downloading the dataset, we can also just use the function 'fetch_openml' from 'sklearn.datasets':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to split the dataset into a train and a test partition. We will use the train dataset to build the model and then use the test set to verify this model. The computation time for the model will depend on the train set size. So please set the size according to your machine's capabilities.\n",
    "\n",
    "(Especially the SVD might take some time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ce414dd22343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# n_train_samples = 50_000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# n_train_samples = 1000\n",
    "# n_train_samples = 2000\n",
    "# n_train_samples = 4000\n",
    "n_train_samples = 10_000\n",
    "# n_train_samples = 20_000\n",
    "# n_train_samples = 50_000\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, train_size=n_train_samples)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, train_size=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-16001c4ebdbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample is represented by a vector of size 28*28=784. Each of these 784 numbers represents the gray-scale value of a pixel in a range from 0 to 255 (8 bit).\n",
    "\n",
    "Let's look at sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(x_train[1].reshape(28,28), cmap=\"gray\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PCA, we first need to 'center' the data by calculating the mean data vector and then subtract this vector from each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = x_train.mean(axis=0)\n",
    "x_train_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an image of the mean data vector\n",
    "plt.matshow(x_train_mean.reshape(28,28), cmap=\"gray\", vmin=0, vmax=255)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_centered = x_train - x_train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the main part of the PCA - the matrix decomposition. There are two equivalent ways to do this. We can either use SVD on the data matrix or use Eigenvalue decomposition on the data covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, Vt = np.linalg.svd(x_train_centered/np.sqrt(len(x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, Vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_svd = s**2\n",
    "modes_svd = Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the 'energy' of the modes compared to the total energy. By finding the intercept with the value of the 'energy criterion' (e.g. 0.95 of the total energy) we can figure out, how many modes we need to faithfully represent our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(lambda_svd)/np.sum(lambda_svd))\n",
    "plt.axhline(0.95, ls=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_svd_sum = np.sum(lambda_svd)\n",
    "lambda_svd_cumsum = np.cumsum(lambda_svd)\n",
    "for i in range(20):\n",
    "    print(i, lambda_svd[i], lambda_svd_cumsum[i]/lambda_svd_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same results can be achieved by using Eigenvalue decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = x_train_centered.T @ x_train_centered / len(x_train_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_eig, modes_eig = np.linalg.eig(C)\n",
    "lambda_eig = lambda_eig.astype(np.float64)\n",
    "modes_eig = modes_eig.T.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(lambda_eig)/np.sum(lambda_eig))\n",
    "plt.axhline(0.95, ls=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the modes are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(modes_svd[0].reshape(28,28))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(modes_eig[0].reshape(28,28))\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.matshow(modes_svd[i].astype(\"double\").reshape(28,28))\n",
    "    plt.colorbar()\n",
    "    plt.gca().set_title(f\"Mode {i} SVD\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(modes_eig[i].astype(\"double\").reshape(28,28))\n",
    "    plt.colorbar()\n",
    "    plt.gca().set_title(f\"Mode {i} Eig\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Except for the +/- sign, these images should be equal for SVD and Eigenvalue decomposition.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the number of modes we need to represent the samples according to the energy criterion (e.g. 0.95):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two methods are equivalent:\n",
    "# (a)\n",
    "r_truncation = np.where((np.cumsum(lambda_eig)/np.sum(lambda_eig)) >= 0.95)[0][0]\n",
    "print(r_truncation)\n",
    "\n",
    "# (b)\n",
    "for i in range(784):\n",
    "    if np.sum(lambda_eig[:i+1]) / np.sum(lambda_eig) >= 0.95:\n",
    "        r_truncation = i\n",
    "        break\n",
    "\n",
    "r_truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the compression works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(x_test[0].reshape(28,28))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compress a data vector by projecting it into a lower dimensional space. We can do this by simply by computing the product with the (truncated) matrix of the modes. To restore the original vector, we then just have to repeat the multiplication and add the mean vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r_truncation\n",
    "x_compressed = x_test[0] @ Vt[:r].T\n",
    "x_restored = x_compressed @ Vt[:r] + x_train_mean\n",
    "x_compressed.shape, x_restored.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(x_restored.reshape(28,28))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the result, we can see that the restored version of the data vector is not a perfect match with the original vector. Some information has been lost. But it should still be a pretty good approximation of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well the lower dimensional representation works with different truncation ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.matshow(x_test[i].reshape(28,28))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "for r in [0, 5, 10, 20, 50, 100, 200, 500, 784]:\n",
    "    #fig, ax = plt.subplots(2, figsize=(10,4))\n",
    "    x_rec = x_test[i] @ modes_svd[:r].T @ modes_svd[:r] + x_train_mean\n",
    "    #x_rec = np.clip(x_rec, 0., 255.)\n",
    "    #plt.matshow(x_rec.reshape(28,28), vmin=0, vmax=255)\n",
    "    plt.matshow(x_rec.reshape(28,28))\n",
    "    #plt.matshow(x_rec.reshape(28,28), cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.colorbar()\n",
    "    plt.gca().set_title(f\"Rank {r} Approximation\")\n",
    "    plt.show()\n",
    "\n",
    "    print(r, np.sum(lambda_svd[:r])/np.sum(lambda_svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower dimensional projection of the data can be used in other downstream tasks. As an example we will now build a very simple Classifier to recognize the handwritten digits. Using the projected data can make downstream tasks more efficient, because of the lower dimensionality of the data.\n",
    "\n",
    "(This demonstration is not related to your assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-97e730b5890c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx_train_compressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mmodes_svd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx_test_compressed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mmodes_svd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "r = 20\n",
    "\n",
    "x_train_compressed = x_train @ modes_svd[:r].T\n",
    "x_test_compressed = x_test @ modes_svd[:r].T\n",
    "\n",
    "reg = SGDClassifier().fit(x_train_compressed, y_train)\n",
    "y_test_pred = reg.predict(x_test_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_compressed.shape, x_test_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many train samples are correctly classified?\n",
    "reg.score(x_train_compressed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many test samples are correctly classified?\n",
    "reg.score(x_test_compressed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.predict(x_test_compressed[:10]))\n",
    "print([y for y in y_test[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are still having troubles understanding PCA, the following lectures by Brunton & Kutz might be helpful for you:\n",
    "\n",
    "http://www.databookuw.com/page-2/page-4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check the follwing paper for applications of PCA/POD in aerospace engineering:\n",
    "\n",
    "http://dx.doi.org/10.14279/depositonce-8512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "46d3e77e92486bfe6c9678dacfe2bef93bc4b19484076a0b7abe74e8327e5887"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
